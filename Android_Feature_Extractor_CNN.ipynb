{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e50fe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import os\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f7b6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareTrainDataset ( raw_train_data , seq_length , word_to_id ) :\n",
    "    train_data = []\n",
    "    for raw_report, label in raw_train_data :\n",
    "        encoded_report = [ word_to_id[t] for t in raw_report ]\n",
    "        padding = seq_length - len(encoded_report)\n",
    "        encoded_report += [ 0 for x in range(padding) ]\n",
    "        train_data.append((encoded_report, label))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterReport ( static_analysis ) :\n",
    "    report = str(static_analysis)\n",
    "    report = report.split(' ')\n",
    "    noise = \"{}()[],':\\\\\" + '\"'\n",
    "    filtered_report = []\n",
    "    for token in report :\n",
    "        refined = token\n",
    "        refined = refined.replace(\"\\n\", '')\n",
    "        for n in noise :\n",
    "            refined = refined.replace(n, '')\n",
    "        filtered_report.append(refined)\n",
    "    return filtered_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dc4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadRawDataset ( directory ) :\n",
    "    malicious_apps_folder = directory + '/malware'\n",
    "    benign_apps_folder = directory + '/benign'\n",
    "    \n",
    "    report_label_pairs = []\n",
    "    \n",
    "    for file_name in [file for file in os.listdir(malicious_apps_folder) if file.endswith('.json')] :\n",
    "        with open(malicious_apps_folder + '/' + file_name) as json_file :\n",
    "            report = json.load(json_file)\n",
    "            filtered_report = FilterReport(report)\n",
    "            report_label_pairs.append((filtered_report, 1))\n",
    "    \n",
    "    for file_name in [file for file in os.listdir(benign_apps_folder) if file.endswith('.json')] :\n",
    "        with open(benign_apps_folder + '/' + file_name) as json_file :\n",
    "            report = json.load(json_file)\n",
    "            filtered_report = FilterReport(report)\n",
    "            report_label_pairs.append((filtered_report, 0))\n",
    "    \n",
    "    return report_label_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe1fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSequenceLength ( train_dataset ) :\n",
    "    max_report_length = 0\n",
    "    for report, label in train_dataset :\n",
    "        if ( len(report) > max_report_length ) :\n",
    "            max_report_length = len(report)\n",
    "    return max_report_length\n",
    "\n",
    "def GetVocabularyLookups ( train_dataset ) :\n",
    "    vocabulary = set()\n",
    "    for report, label in train_dataset :\n",
    "        vocabulary = vocabulary.union(report)\n",
    "    vocabulary.add('')\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary.sort()\n",
    "    \n",
    "    ID_TO_WORD = vocabulary\n",
    "    WORD_TO_ID = dict(list(enumerate(ID_TO_WORD)))\n",
    "    WORD_TO_ID = { value : key for key, value in WORD_TO_ID.items() }\n",
    "    \n",
    "    return ID_TO_WORD, WORD_TO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71f5bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeBatches ( train_data , batch_size = 64 ) :\n",
    "    shuffle(train_data)\n",
    "    batches = list()\n",
    "    for start in range(0, len(train_data), batch_size) :\n",
    "        end = start + batch_size\n",
    "        batch, labels = list(zip(*train_data[start:end]))\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        batch = torch.Tensor(batch).long()\n",
    "        batches.append((batch, labels))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c00b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Android_Feature_Extractor_CNN ( nn.Module ) :\n",
    "    def __init__ ( self , vocab_size , seq_length , feature_length , \n",
    "                   word_embed_size = 152 , filter_sizes = (2, 3, 4) ) :\n",
    "        super(Android_Feature_Extractor_CNN, self).__init__()\n",
    "        \n",
    "        self.word_embed_size = word_embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.feature_length = feature_length\n",
    "        \n",
    "        self.word_embed = nn.Embedding ( vocab_size, word_embed_size )\n",
    "        self.conv = [\n",
    "            nn.Conv1d ( word_embed_size , word_embed_size // 2 , fs , stride = 1 ) for fs in filter_sizes\n",
    "        ]\n",
    "        self.pool = nn.MaxPool1d ( 5 , stride = 3 )\n",
    "        \n",
    "        feature_length_after_cnn_layers = self.cnn_feature_length()\n",
    "        \n",
    "        self.dense_features = nn.Linear ( feature_length_after_cnn_layers , feature_length )\n",
    "        self.drop = nn.Dropout ( p = 0.25 )\n",
    "        self.dense_classes = nn.Linear ( feature_length , 2 )\n",
    "    \n",
    "    def cnn_feature_length ( self ) :\n",
    "        dummy_inputs = torch.randn(1, self.seq_length, self.word_embed_size)\n",
    "        dummy_inputs = dummy_inputs.reshape(1, self.word_embed_size, self.seq_length)\n",
    "        \n",
    "        convolved_inputs = []\n",
    "        for layer in self.conv :\n",
    "            convolved_inputs.append( layer(dummy_inputs) )\n",
    "        \n",
    "        pooled_convolved_inputs = []\n",
    "        for conv_in in convolved_inputs :\n",
    "            pooled_convolved_inputs.append( self.pool(torch.transpose(conv_in, 1, 2)) )\n",
    "        \n",
    "        flattened_feature_vectors = []\n",
    "        for p in pooled_convolved_inputs :\n",
    "            flattened_feature_vectors.append(p.reshape(1, -1))\n",
    "        \n",
    "        return torch.cat(flattened_feature_vectors, axis = 1)[0].shape[0]\n",
    "    \n",
    "    def forward ( self , inputs ) :\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = self.word_embed(inputs)\n",
    "        inputs = torch.transpose(inputs, 1, 2)\n",
    "        \n",
    "        convolved_inputs = []\n",
    "        for layer in self.conv :\n",
    "            convolved_inputs.append( torch.relu(layer(inputs)) )\n",
    "        \n",
    "        pooled_convolved_inputs = []\n",
    "        for conv in convolved_inputs :\n",
    "            pooled_convolved_inputs.append( self.pool(torch.transpose(conv, 1, 2)) )\n",
    "        \n",
    "        cnn_features = []\n",
    "        for pool in pooled_convolved_inputs :\n",
    "            cnn_features.append( pool.reshape(batch_size, -1) )\n",
    "        \n",
    "        cnn_features = torch.cat(cnn_features, axis = 1)\n",
    "        \n",
    "        features = self.dense_features(cnn_features)\n",
    "        features = torch.tanh(features)\n",
    "        dropped_features = self.drop(features)\n",
    "        \n",
    "        classes = self.dense_classes(dropped_features)\n",
    "        classes = torch.sigmoid(classes)\n",
    "        \n",
    "        return features, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel ( train_data , total_epochs , learning_rate = 0.01 , batch_size = 64 ) :\n",
    "    global FE_CNN\n",
    "    optimizer = torch.optim.Adam(FE_CNN.parameters(), lr = learning_rate)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(total_epochs) :\n",
    "        print('\\n EPOCH {} STARTED '.format(epoch+1))\n",
    "        total_loss = 0.0\n",
    "        batches = MakeBatches(train_data, batch_size)\n",
    "        epoch_start_time = time()\n",
    "        \n",
    "        for step, (batch, labels) in enumerate(batches) :\n",
    "            step_start_time = time()\n",
    "            _, malware_classification = FE_CNN(batch)\n",
    "            \n",
    "            classification_loss = loss(malware_classification, labels)\n",
    "            total_loss += classification_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            classification_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ti = time() - step_start_time\n",
    "            print('    STEP : {:3d} | LOSS : {:.6f} | DUR : {:.4f}'.format(step+1, classification_loss, ti))\n",
    "        \n",
    "        classification_loss = total_loss / len(batches)\n",
    "        ti = time() - epoch_start_time\n",
    "        print(' EPOCH\\'S MEAN LOSS : {:.6f} | DUR : {:.4f}'.format(classification_loss, ti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cae095",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TRAIN_DATA = LoadRawDataset('/Users/nakulaggarwal/Documents/UNBInternship/implementation/data')\n",
    "SEQ_LENGTH = GetSequenceLength(RAW_TRAIN_DATA)\n",
    "ID_TO_WORD, WORD_TO_ID = GetVocabularyLookups(RAW_TRAIN_DATA)\n",
    "TRAIN_DATA = PrepareTrainDataset(RAW_TRAIN_DATA, SEQ_LENGTH, WORD_TO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fde76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(ID_TO_WORD)\n",
    "FEATURE_LENGTH = 64\n",
    "FE_CNN = Android_Feature_Extractor_CNN(VOCAB_SIZE, SEQ_LENGTH, FEATURE_LENGTH, filter_sizes = (2, 3, 4, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed589d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainModel(TRAIN_DATA, 100, 0.001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca22649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def CopyAllMalwareFiles ( source_directory , dest_directory , first_id ) :\n",
    "    folders = [ source_directory + '/' + folder for folder in os.listdir(source_directory) ]\n",
    "    idd = first_id\n",
    "    for folder in folders :\n",
    "        try : json_files = [file for file in os.listdir(folder) if file.endswith('static.json')]\n",
    "        except : continue\n",
    "        json_file = folder + '/' + json_files[0]\n",
    "        shutil.copyfile(json_file, dest_directory + '/' + str(idd) + '.json')\n",
    "        idd += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffeb5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CopyAllMalwareFiles('data/banking', 'data/malware', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02811475",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
