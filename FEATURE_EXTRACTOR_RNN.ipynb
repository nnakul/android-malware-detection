{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import os\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrepareTrainDataset ( raw_train_data , seq_length , word_to_id ) :\n",
    "    train_data = []\n",
    "    for raw_report, label in raw_train_data :\n",
    "        encoded_report = [ word_to_id[t] for t in raw_report ]\n",
    "        padding = seq_length - len(encoded_report)\n",
    "        encoded_report += [ 0 for x in range(padding) ]\n",
    "        train_data.append((encoded_report, label))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterReport ( static_analysis ) :\n",
    "    report = str(static_analysis)\n",
    "    report = report.split(' ')\n",
    "    noise = \"{}()[],':\\\\\" + '\"'\n",
    "    filtered_report = []\n",
    "    for token in report :\n",
    "        refined = token\n",
    "        refined = refined.replace(\"\\n\", '')\n",
    "        for n in noise :\n",
    "            refined = refined.replace(n, '')\n",
    "        filtered_report.append(refined)\n",
    "    return filtered_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadRawDataset ( directory ) :\n",
    "    malicious_apps_folder = directory + '/malware'\n",
    "    benign_apps_folder = directory + '/benign'\n",
    "    \n",
    "    report_label_pairs = []\n",
    "    \n",
    "    for file_name in [file for file in os.listdir(malicious_apps_folder) if file.endswith('.json')] :\n",
    "        with open(malicious_apps_folder + '/' + file_name) as json_file :\n",
    "            report = json.load(json_file)\n",
    "            filtered_report = FilterReport(report)\n",
    "            report_label_pairs.append((filtered_report, 1))\n",
    "    \n",
    "    for file_name in [file for file in os.listdir(benign_apps_folder) if file.endswith('.json')] :\n",
    "        with open(benign_apps_folder + '/' + file_name) as json_file :\n",
    "            report = json.load(json_file)\n",
    "            filtered_report = FilterReport(report)\n",
    "            report_label_pairs.append((filtered_report, 0))\n",
    "    \n",
    "    return report_label_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSequenceLength ( train_dataset ) :\n",
    "    max_report_length = 0\n",
    "    for report, label in train_dataset :\n",
    "        if ( len(report) > max_report_length ) :\n",
    "            max_report_length = len(report)\n",
    "    return max_report_length\n",
    "\n",
    "def GetVocabularyLookups ( train_dataset ) :\n",
    "    vocabulary = set()\n",
    "    for report, label in train_dataset :\n",
    "        vocabulary = vocabulary.union(report)\n",
    "    vocabulary.add('')\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary.sort()\n",
    "    \n",
    "    ID_TO_WORD = vocabulary\n",
    "    WORD_TO_ID = dict(list(enumerate(ID_TO_WORD)))\n",
    "    WORD_TO_ID = { value : key for key, value in WORD_TO_ID.items() }\n",
    "    \n",
    "    return ID_TO_WORD, WORD_TO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeBatches ( train_data , batch_size = 64 ) :\n",
    "    shuffle(train_data)\n",
    "    batches = list()\n",
    "    for start in range(0, len(train_data), batch_size) :\n",
    "        end = start + batch_size\n",
    "        batch, labels = list(zip(*train_data[start:end]))\n",
    "        labels = torch.Tensor(labels).long()\n",
    "        batch = torch.Tensor(batch).long()\n",
    "        batches.append((batch, labels))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Android_Feature_Extractor_RNN ( nn.Module ) :\n",
    "    def __init__ ( self , vocab_size , seq_length , feature_length , \n",
    "                   word_embed_size = 152 , hidden_size = 128 , num_layers = 2 ) :\n",
    "        super(Android_Feature_Extractor_RNN, self).__init__()\n",
    "        \n",
    "        self.word_embed_size = word_embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.feature_length = feature_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.word_embed = nn.Embedding ( vocab_size, word_embed_size )\n",
    "        self.lstm = nn.LSTM ( word_embed_size , hidden_size , num_layers = num_layers , \n",
    "                              batch_first = True , dropout = 0.2 )\n",
    "        self.dense = nn.Linear ( hidden_size , feature_length )\n",
    "        self.feature_drop = nn.Dropout ( p = 0.3 )\n",
    "        self.feature_dense = nn.Linear ( feature_length , feature_length )\n",
    "        self.class_drop = nn.Dropout ( p = 0.3 )\n",
    "        self.class_dense = nn.Linear ( feature_length , 2 )\n",
    "    \n",
    "    def forward ( self , inputs ) :\n",
    "        orig_inputs = inputs\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = self.word_embed(inputs)\n",
    "        \n",
    "        h_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c_0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        lstm_outputs, _ = self.lstm(inputs, (h_0, c_0))\n",
    "        \n",
    "        masking_indices = [ np.argmax(orig_inputs[x] == 0) - 1 for x in range(batch_size) ]\n",
    "        lstm_outputs = torch.stack( [ lstm_outputs[idd, k, :] for idd, k in enumerate(masking_indices) ] )\n",
    "        \n",
    "        scaled_lstm_outs = self.dense(lstm_outputs)\n",
    "        \n",
    "        dropped_lstm_outs = self.feature_drop(scaled_lstm_outs)\n",
    "        features = self.feature_dense(dropped_lstm_outs)\n",
    "        \n",
    "        features_dropped = self.class_drop(features)\n",
    "        classes = self.class_dense(features_dropped)\n",
    "        \n",
    "        return features, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainModel ( train_data , total_epochs , learning_rate = 0.01 , batch_size = 64 ) :\n",
    "    global FE_RNN\n",
    "    optimizer = torch.optim.Adam(FE_RNN.parameters(), lr = learning_rate)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(total_epochs) :\n",
    "        print('\\n EPOCH {} STARTED '.format(epoch+1))\n",
    "        total_loss = 0.0\n",
    "        batches = MakeBatches(train_data, batch_size)\n",
    "        epoch_start_time = time()\n",
    "        \n",
    "        for step, (batch, labels) in enumerate(batches) :\n",
    "            step_start_time = time()\n",
    "            _, malware_classification = FE_RNN(batch)\n",
    "            \n",
    "            classification_loss = loss(malware_classification, labels)\n",
    "            total_loss += classification_loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            classification_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            ti = time() - step_start_time\n",
    "            print('    STEP : {:3d} | LOSS : {:.6f} | DUR : {:.4f}'.format(step+1, classification_loss, ti))\n",
    "        \n",
    "        classification_loss = total_loss / len(batches)\n",
    "        ti = time() - epoch_start_time\n",
    "        print(' EPOCH\\'S MEAN LOSS : {:.6f} | DUR : {:.4f}'.format(classification_loss, ti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TRAIN_DATA = LoadRawDataset('/Users/nakulaggarwal/Documents/UNBInternship/implementation/data')\n",
    "SEQ_LENGTH = GetSequenceLength(RAW_TRAIN_DATA)\n",
    "ID_TO_WORD, WORD_TO_ID = GetVocabularyLookups(RAW_TRAIN_DATA)\n",
    "TRAIN_DATA = PrepareTrainDataset(RAW_TRAIN_DATA, SEQ_LENGTH, WORD_TO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(ID_TO_WORD)\n",
    "FEATURE_LENGTH = 64\n",
    "FE_RNN = Android_Feature_Extractor_RNN(VOCAB_SIZE, SEQ_LENGTH, FEATURE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TrainModel(TRAIN_DATA, 10, 0.001, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
